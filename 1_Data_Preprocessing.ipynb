{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openml\n",
    "import os\n",
    "import zipfile\n",
    "# Load the data\n",
    "import pandas as pd\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "import pickle \n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading and cleaning each dataset one by one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize master dictionary\n",
    "all_datasets = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fri_c1_500_50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = openml.datasets.get_dataset(637)\n",
    "X, y, is_categorical, feat_names = dataset.get_data(\n",
    "    dataset_format=\"dataframe\", target=dataset.default_target_attribute\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_datasets[\"fri_c1_500_50\"] = {\"X\": X, \"y\": y}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fri_c3_1000_50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = openml.datasets.get_dataset(618)\n",
    "X, y, is_categorical, feat_names = dataset.get_data(\n",
    "    dataset_format=\"dataframe\", target=dataset.default_target_attribute\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_datasets[\"fri_c3_1000_50\"] = {\"X\": X, \"y\": y}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fri_c4_500_50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = openml.datasets.get_dataset(616)\n",
    "X, y, is_categorical, feat_names = dataset.get_data(\n",
    "    dataset_format=\"dataframe\", target=dataset.default_target_attribute\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_datasets[\"fri_c4_500_50\"] = {\"X\": X, \"y\": y}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fri_c4_1000_50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = openml.datasets.get_dataset(607)\n",
    "X, y, is_categorical, feat_names = dataset.get_data(\n",
    "    dataset_format=\"dataframe\", target=dataset.default_target_attribute\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_datasets[\"fri_c4_1000_50\"] = {\"X\": X, \"y\": y}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " fri_c2_1000_25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = openml.datasets.get_dataset(589)\n",
    "X, y, is_categorical, feat_names = dataset.get_data(\n",
    "    dataset_format=\"dataframe\", target=dataset.default_target_attribute\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_datasets[\"fri_c2_1000_25\"] = {\"X\": X, \"y\": y}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " fri_c1_1000_25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = openml.datasets.get_dataset(620)\n",
    "X, y, is_categorical, feat_names = dataset.get_data(\n",
    "    dataset_format=\"dataframe\", target=dataset.default_target_attribute\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_datasets[\"fri_c1_1000_25\"] = {\"X\": X, \"y\": y}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fri_c3_1000_25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = openml.datasets.get_dataset(586)\n",
    "X, y, is_categorical, feat_names = dataset.get_data(\n",
    "    dataset_format=\"dataframe\", target=dataset.default_target_attribute\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_datasets[\"fri_c3_1000_25\"] = {\"X\": X, \"y\": y}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Body Fat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the environment variable to the folder (not the full file path!)\n",
    "os.environ['KAGGLE_CONFIG_DIR'] = r'C:\\Users\\25644574\\OneDrive - UTS\\Desktop\\Optimal_K_Selection'\n",
    "# Download the data\n",
    "!kaggle datasets download -d fedesoriano/body-fat-prediction-dataset\n",
    "with zipfile.ZipFile(\"body-fat-prediction-dataset.zip\", 'r') as zip_ref:\n",
    "    zip_ref.extractall(\"your_directory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"your_directory/bodyfat.csv\")  # Replace with actual filename if different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values:\n",
      " Series([], dtype: int64)\n"
     ]
    }
   ],
   "source": [
    "#Check for missing values\n",
    "missing = df.isnull().sum()\n",
    "missing = missing[missing > 0]\n",
    "print(\"Missing values:\\n\", missing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Density    float64\n",
      "BodyFat    float64\n",
      "Age          int64\n",
      "Weight     float64\n",
      "Height     float64\n",
      "Neck       float64\n",
      "Chest      float64\n",
      "Abdomen    float64\n",
      "Hip        float64\n",
      "Thigh      float64\n",
      "Knee       float64\n",
      "Ankle      float64\n",
      "Biceps     float64\n",
      "Forearm    float64\n",
      "Wrist      float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Print the data types of each column\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define target and features\n",
    "y = df[\"BodyFat\"]     # Target variable\n",
    "X = df.drop(\"BodyFat\", axis=1)  # Features (all columns except target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_datasets[\"BodyFat\"] = {\"X\": X, \"y\": y}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forest Fires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'uci_id': 162, 'name': 'Forest Fires', 'repository_url': 'https://archive.ics.uci.edu/dataset/162/forest+fires', 'data_url': 'https://archive.ics.uci.edu/static/public/162/data.csv', 'abstract': 'This is a difficult regression task, where the aim is to predict the burned area of forest fires, in the northeast region of Portugal, by using meteorological and other data (see details at: http://www.dsi.uminho.pt/~pcortez/forestfires).', 'area': 'Climate and Environment', 'tasks': ['Regression'], 'characteristics': ['Multivariate'], 'num_instances': 517, 'num_features': 12, 'feature_types': ['Real'], 'demographics': [], 'target_col': ['area'], 'index_col': None, 'has_missing_values': 'no', 'missing_values_symbol': None, 'year_of_dataset_creation': 2007, 'last_updated': 'Thu Jan 11 2024', 'dataset_doi': '10.24432/C5D88D', 'creators': ['Paulo Cortez', 'Anbal Morais'], 'intro_paper': {'ID': 368, 'type': 'NATIVE', 'title': 'A data mining approach to predict forest fires using meteorological data', 'authors': 'P. Cortez, AnÃ­bal de Jesus Raimundo Morais', 'venue': 'New Trends in Artificial Intelligence, Proceedings of the 13th EPIA 2007 - Portuguese Conference on Artificial Intelligence', 'year': 2007, 'journal': None, 'DOI': None, 'URL': 'https://www.semanticscholar.org/paper/A-data-mining-approach-to-predict-forest-fires-data-Cortez-Morais/0f529dc2b2b2bad22394454d4cba79e2c319f0b0', 'sha': None, 'corpus': None, 'arxiv': None, 'mag': None, 'acl': None, 'pmid': None, 'pmcid': None}, 'additional_info': {'summary': \"In [Cortez and Morais, 2007], the output 'area' was first transformed with a ln(x+1) function.\\r\\n   Then, several Data Mining methods were applied. After fitting the models, the outputs were\\r\\n   post-processed with the inverse of the ln(x+1) transform. Four different input setups were\\r\\n   used. The experiments were conducted using a 10-fold (cross-validation) x 30 runs. Two\\r\\n   regression metrics were measured: MAD and RMSE. A Gaussian support vector machine (SVM) fed\\r\\n   with only 4 direct weather conditions (temp, RH, wind and rain) obtained the best MAD value:\\r\\n   12.71 +- 0.01 (mean and confidence interval within 95% using a t-student distribution). The\\r\\n   best RMSE was attained by the naive mean predictor. An analysis to the regression error curve\\r\\n   (REC) shows that the SVM model predicts more examples within a lower admitted error. In effect,\\r\\n   the SVM model predicts better small fires, which are the majority. \", 'purpose': None, 'funded_by': None, 'instances_represent': None, 'recommended_data_splits': None, 'sensitive_data': None, 'preprocessing_description': None, 'variable_info': \"For more information, read [Cortez and Morais, 2007].\\r\\n   1. X - x-axis spatial coordinate within the Montesinho park map: 1 to 9\\r\\n   2. Y - y-axis spatial coordinate within the Montesinho park map: 2 to 9\\r\\n   3. month - month of the year: 'jan' to 'dec' \\r\\n   4. day - day of the week: 'mon' to 'sun'\\r\\n   5. FFMC - FFMC index from the FWI system: 18.7 to 96.20\\r\\n   6. DMC - DMC index from the FWI system: 1.1 to 291.3 \\r\\n   7. DC - DC index from the FWI system: 7.9 to 860.6 \\r\\n   8. ISI - ISI index from the FWI system: 0.0 to 56.10\\r\\n   9. temp - temperature in Celsius degrees: 2.2 to 33.30\\r\\n   10. RH - relative humidity in %: 15.0 to 100\\r\\n   11. wind - wind speed in km/h: 0.40 to 9.40 \\r\\n   12. rain - outside rain in mm/m2 : 0.0 to 6.4 \\r\\n   13. area - the burned area of the forest (in ha): 0.00 to 1090.84 \\r\\n   (this output variable is very skewed towards 0.0, thus it may make\\r\\n    sense to model with the logarithm transform).\", 'citation': None}}\n",
      "     name     role         type demographic  \\\n",
      "0       X  Feature      Integer        None   \n",
      "1       Y  Feature      Integer        None   \n",
      "2   month  Feature  Categorical        None   \n",
      "3     day  Feature  Categorical        None   \n",
      "4    FFMC  Feature   Continuous        None   \n",
      "5     DMC  Feature      Integer        None   \n",
      "6      DC  Feature   Continuous        None   \n",
      "7     ISI  Feature   Continuous        None   \n",
      "8    temp  Feature   Continuous        None   \n",
      "9      RH  Feature      Integer        None   \n",
      "10   wind  Feature   Continuous        None   \n",
      "11   rain  Feature      Integer        None   \n",
      "12   area   Target      Integer        None   \n",
      "\n",
      "                                          description            units  \\\n",
      "0   x-axis spatial coordinate within the Montesinh...             None   \n",
      "1   y-axis spatial coordinate within the Montesinh...             None   \n",
      "2                  month of the year: 'jan' to 'dec'              None   \n",
      "3                     day of the week: 'mon' to 'sun'             None   \n",
      "4       FFMC index from the FWI system: 18.7 to 96.20             None   \n",
      "5        DMC index from the FWI system: 1.1 to 291.3              None   \n",
      "6          DC index from the FWI system: 7.9 to 860.6             None   \n",
      "7         ISI index from the FWI system: 0.0 to 56.10             None   \n",
      "8                           temperature: 2.2 to 33.30  Celsius degrees   \n",
      "9                      relative humidity: 15.0 to 100                %   \n",
      "10                           wind speed: 0.40 to 9.40             km/h   \n",
      "11                          outside rain: 0.0 to 6.4             mm/m2   \n",
      "12  the burned area of the forest: 0.00 to 1090.84...               ha   \n",
      "\n",
      "   missing_values  \n",
      "0              no  \n",
      "1              no  \n",
      "2              no  \n",
      "3              no  \n",
      "4              no  \n",
      "5              no  \n",
      "6              no  \n",
      "7              no  \n",
      "8              no  \n",
      "9              no  \n",
      "10             no  \n",
      "11             no  \n",
      "12             no  \n"
     ]
    }
   ],
   "source": [
    "# fetch dataset \n",
    "forest_fires = fetch_ucirepo(id=162) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "X = forest_fires.data.features \n",
    "y = forest_fires.data.targets \n",
    "  \n",
    "# metadata \n",
    "print(forest_fires.metadata) \n",
    "  \n",
    "# variable information \n",
    "print(forest_fires.variables) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_datasets[\"Forest_Fires\"] = {\"X\": X, \"y\": y}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0.00/17.1k [00:00<?, ?B/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17.1k/17.1k [00:00<00:00, 4.75MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset URL: https://www.kaggle.com/datasets/mathurinache/quakes\n",
      "License(s): CC0-1.0\n",
      "Downloading quakes.zip to c:\\Users\\25644574\\OneDrive - UTS\\Desktop\\Optimal_K_Selection\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!kaggle datasets download -d mathurinache/quakes\n",
    "with zipfile.ZipFile(\"quakes.zip\", 'r') as zip_ref:\n",
    "    zip_ref.extractall(\"your_directory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"your_directory/dataset_2195_quake.csv\")  # Replace with actual filename if different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values:\n",
      " Series([], dtype: int64)\n"
     ]
    }
   ],
   "source": [
    "#Check for missing values\n",
    "missing = df.isnull().sum()\n",
    "missing = missing[missing > 0]\n",
    "print(\"Missing values:\\n\", missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "focal_depth      int64\n",
      "latitude       float64\n",
      "longitude      float64\n",
      "richter        float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Print the data types of each column\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define target and features\n",
    "y = df[\"richter\"]\n",
    "X = df.drop(\"richter\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_datasets[\"Quakes\"] = {\"X\": X, \"y\": y}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Servo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'uci_id': 87, 'name': 'Servo', 'repository_url': 'https://archive.ics.uci.edu/dataset/87/servo', 'data_url': 'https://archive.ics.uci.edu/static/public/87/data.csv', 'abstract': 'Data was from a simulation of a servo system', 'area': 'Computer Science', 'tasks': ['Regression'], 'characteristics': ['Multivariate'], 'num_instances': 167, 'num_features': 4, 'feature_types': ['Categorical', 'Integer'], 'demographics': [], 'target_col': ['class'], 'index_col': None, 'has_missing_values': 'no', 'missing_values_symbol': None, 'year_of_dataset_creation': 1986, 'last_updated': 'Fri Mar 08 2024', 'dataset_doi': '10.24432/C5Q30F', 'creators': ['Karl Ulrich'], 'intro_paper': None, 'additional_info': {'summary': 'Ross Quinlan:\\r\\n\\r\\nThis data was given to me by Karl Ulrich at MIT in 1986.  I didn\\'t record his description at the time, but here\\'s his subsequent (1992) recollection:\\r\\n \\r\\n\"I seem to remember that the data was from a simulation of a servo system involving a servo amplifier, a motor, a lead screw/nut, and a sliding carriage of some sort.  It may have been on of the translational axes of a robot on the 9th floor of the AI lab.  In any case, the output value is almost certainly a rise time, or the time required for the system to respond to a step change in a position set point.\"\\r\\n \\r\\n(Quinlan, ML\\'93)\\r\\n\\r\\n\"This is an interesting collection of data provided by Karl Ulrich.  It covers an extremely non-linear phenomenon - predicting the rise time of a servomechanism in terms of two (continuous) gain settings and two (discrete) choices of mechanical linkages.\"', 'purpose': None, 'funded_by': None, 'instances_represent': None, 'recommended_data_splits': None, 'sensitive_data': None, 'preprocessing_description': None, 'variable_info': '   1. motor: A,B,C,D,E\\r\\n   2. screw: A,B,C,D,E\\r\\n   3. pgain: 3,4,5,6\\r\\n   4. vgain: 1,2,3,4,5\\r\\n   5. class: 0.13 to 7.10', 'citation': None}}\n",
      "    name     role         type demographic description units missing_values\n",
      "0  motor  Feature  Categorical        None        None  None             no\n",
      "1  screw  Feature  Categorical        None        None  None             no\n",
      "2  pgain  Feature      Integer        None        None  None             no\n",
      "3  vgain  Feature      Integer        None        None  None             no\n",
      "4  class   Target   Continuous        None        None  None             no\n"
     ]
    }
   ],
   "source": [
    "# fetch dataset \n",
    "servo = fetch_ucirepo(id=87) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "X = servo.data.features \n",
    "y = servo.data.targets \n",
    "  \n",
    "# metadata \n",
    "print(servo.metadata) \n",
    "  \n",
    "# variable information \n",
    "print(servo.variables) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_datasets[\"Servo\"] = {\"X\": X, \"y\": y}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "auto93"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = openml.datasets.get_dataset(569)\n",
    "X, y, is_categorical, feat_names = dataset.get_data(\n",
    "    dataset_format=\"dataframe\", target=dataset.default_target_attribute\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with missing values:\n",
      "Number_of_cylinders     1\n",
      "Rear_seat_room          2\n",
      "Luggage_capacity       11\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check which variables have missing values\n",
    "missing = X.isnull().sum()\n",
    "missing = missing[missing > 0]\n",
    "\n",
    "if not missing.empty:\n",
    "    print(\"Columns with missing values:\")\n",
    "    print(missing)\n",
    "else:\n",
    "    print(\"No missing values in X.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute the missing values\n",
    "X[\"Number_of_cylinders\"] = X[\"Number_of_cylinders\"].fillna(X[\"Number_of_cylinders\"].mean())\n",
    "X[\"Rear_seat_room\"] = X[\"Rear_seat_room\"].fillna(X[\"Rear_seat_room\"].mean())\n",
    "X[\"Luggage_capacity\"] = X[\"Luggage_capacity\"].fillna(X[\"Luggage_capacity\"].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_datasets[\"auto93\"] = {\"X\": X, \"y\": y}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "autoPrice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = openml.datasets.get_dataset(207)\n",
    "X, y, is_categorical, feat_names = dataset.get_data(\n",
    "    dataset_format=\"dataframe\", target=dataset.default_target_attribute\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_datasets[\"autoPrice\"] = {\"X\": X, \"y\": y}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Auto MPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'uci_id': 9, 'name': 'Auto MPG', 'repository_url': 'https://archive.ics.uci.edu/dataset/9/auto+mpg', 'data_url': 'https://archive.ics.uci.edu/static/public/9/data.csv', 'abstract': 'Revised from CMU StatLib library, data concerns city-cycle fuel consumption', 'area': 'Other', 'tasks': ['Regression'], 'characteristics': ['Multivariate'], 'num_instances': 398, 'num_features': 7, 'feature_types': ['Real', 'Categorical', 'Integer'], 'demographics': [], 'target_col': ['mpg'], 'index_col': ['car_name'], 'has_missing_values': 'yes', 'missing_values_symbol': 'NaN', 'year_of_dataset_creation': 1993, 'last_updated': 'Thu Aug 10 2023', 'dataset_doi': '10.24432/C5859H', 'creators': ['R. Quinlan'], 'intro_paper': None, 'additional_info': {'summary': 'This dataset is a slightly modified version of the dataset provided in the StatLib library.  In line with the use by Ross Quinlan (1993) in predicting the attribute \"mpg\", 8 of the original instances were removed because they had unknown values for the \"mpg\" attribute.  The original dataset is available in the file \"auto-mpg.data-original\".\\r\\n\\r\\n\"The data concerns city-cycle fuel consumption in miles per gallon, to be predicted in terms of 3 multivalued discrete and 5 continuous attributes.\" (Quinlan, 1993)', 'purpose': None, 'funded_by': None, 'instances_represent': None, 'recommended_data_splits': None, 'sensitive_data': None, 'preprocessing_description': None, 'variable_info': '    1. mpg:           continuous\\r\\n    2. cylinders:     multi-valued discrete\\r\\n    3. displacement:  continuous\\r\\n    4. horsepower:    continuous\\r\\n    5. weight:        continuous\\r\\n    6. acceleration:  continuous\\r\\n    7. model year:    multi-valued discrete\\r\\n    8. origin:        multi-valued discrete\\r\\n    9. car name:      string (unique for each instance)', 'citation': None}}\n",
      "           name     role         type demographic description units  \\\n",
      "0  displacement  Feature   Continuous        None        None  None   \n",
      "1           mpg   Target   Continuous        None        None  None   \n",
      "2     cylinders  Feature      Integer        None        None  None   \n",
      "3    horsepower  Feature   Continuous        None        None  None   \n",
      "4        weight  Feature   Continuous        None        None  None   \n",
      "5  acceleration  Feature   Continuous        None        None  None   \n",
      "6    model_year  Feature      Integer        None        None  None   \n",
      "7        origin  Feature      Integer        None        None  None   \n",
      "8      car_name       ID  Categorical        None        None  None   \n",
      "\n",
      "  missing_values  \n",
      "0             no  \n",
      "1             no  \n",
      "2             no  \n",
      "3            yes  \n",
      "4             no  \n",
      "5             no  \n",
      "6             no  \n",
      "7             no  \n",
      "8             no  \n"
     ]
    }
   ],
   "source": [
    "# fetch dataset \n",
    "auto_mpg = fetch_ucirepo(id=9) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "X = auto_mpg.data.features \n",
    "y = auto_mpg.data.targets \n",
    "  \n",
    "# metadata \n",
    "print(auto_mpg.metadata) \n",
    "  \n",
    "# variable information \n",
    "print(auto_mpg.variables) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Impute the missing values\n",
    "X.loc[:, \"horsepower\"] = X[\"horsepower\"].fillna(X[\"horsepower\"].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_datasets[\"autoMPG\"] = {\"X\": X, \"y\": y}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concrete Compressive Strength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'uci_id': 165, 'name': 'Concrete Compressive Strength', 'repository_url': 'https://archive.ics.uci.edu/dataset/165/concrete+compressive+strength', 'data_url': 'https://archive.ics.uci.edu/static/public/165/data.csv', 'abstract': 'Concrete is the most important material in civil engineering. The concrete compressive strength is a highly nonlinear function of age and ingredients. ', 'area': 'Physics and Chemistry', 'tasks': ['Regression'], 'characteristics': ['Multivariate'], 'num_instances': 1030, 'num_features': 8, 'feature_types': ['Real'], 'demographics': [], 'target_col': ['Concrete compressive strength'], 'index_col': None, 'has_missing_values': 'no', 'missing_values_symbol': None, 'year_of_dataset_creation': 1998, 'last_updated': 'Sun Feb 11 2024', 'dataset_doi': '10.24432/C5PK67', 'creators': ['I-Cheng Yeh'], 'intro_paper': {'ID': 383, 'type': 'NATIVE', 'title': 'Modeling of strength of high-performance concrete using artificial neural networks', 'authors': 'I. Yeh', 'venue': 'Cement and Concrete Research, Vol. 28, No. 12', 'year': 1998, 'journal': None, 'DOI': '10.1016/S0008-8846(98)00165-3', 'URL': 'https://www.semanticscholar.org/paper/9310cae70452ea11465f338483e79cc36a68881c', 'sha': None, 'corpus': None, 'arxiv': None, 'mag': None, 'acl': None, 'pmid': None, 'pmcid': None}, 'additional_info': {'summary': 'Number of instances \\t1030\\r\\nNumber of Attributes\\t9\\r\\nAttribute breakdown\\t8 quantitative input variables, and 1 quantitative output variable\\r\\nMissing Attribute Values\\tNone \\r\\n', 'purpose': None, 'funded_by': None, 'instances_represent': None, 'recommended_data_splits': None, 'sensitive_data': None, 'preprocessing_description': None, 'variable_info': 'Given are the variable name, variable type, the measurement unit and a brief description. The concrete compressive strength is the regression problem. The order of this listing corresponds to the order of numerals along the rows of the database. \\r\\n\\r\\nName -- Data Type -- Measurement -- Description\\r\\n\\r\\nCement (component 1) -- quantitative -- kg in a m3 mixture -- Input Variable\\r\\nBlast Furnace Slag (component 2) -- quantitative -- kg in a m3 mixture -- Input Variable\\r\\nFly Ash (component 3) -- quantitative  -- kg in a m3 mixture -- Input Variable\\r\\nWater  (component 4) -- quantitative  -- kg in a m3 mixture -- Input Variable\\r\\nSuperplasticizer (component 5) -- quantitative -- kg in a m3 mixture -- Input Variable\\r\\nCoarse Aggregate  (component 6) -- quantitative -- kg in a m3 mixture -- Input Variable\\r\\nFine Aggregate (component 7)\\t -- quantitative  -- kg in a m3 mixture -- Input Variable\\r\\nAge -- quantitative  -- Day (1~365) -- Input Variable\\r\\nConcrete compressive strength -- quantitative -- MPa -- Output Variable\\r\\n\\r\\n', 'citation': None}}\n",
      "                            name     role        type demographic description  \\\n",
      "0                         Cement  Feature  Continuous        None        None   \n",
      "1             Blast Furnace Slag  Feature     Integer        None        None   \n",
      "2                        Fly Ash  Feature  Continuous        None        None   \n",
      "3                          Water  Feature  Continuous        None        None   \n",
      "4               Superplasticizer  Feature  Continuous        None        None   \n",
      "5               Coarse Aggregate  Feature  Continuous        None        None   \n",
      "6                 Fine Aggregate  Feature  Continuous        None        None   \n",
      "7                            Age  Feature     Integer        None        None   \n",
      "8  Concrete compressive strength   Target  Continuous        None        None   \n",
      "\n",
      "    units missing_values  \n",
      "0  kg/m^3             no  \n",
      "1  kg/m^3             no  \n",
      "2  kg/m^3             no  \n",
      "3  kg/m^3             no  \n",
      "4  kg/m^3             no  \n",
      "5  kg/m^3             no  \n",
      "6  kg/m^3             no  \n",
      "7     day             no  \n",
      "8     MPa             no  \n"
     ]
    }
   ],
   "source": [
    "# fetch dataset \n",
    "concrete_compressive_strength = fetch_ucirepo(id=165) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "X = concrete_compressive_strength.data.features \n",
    "y = concrete_compressive_strength.data.targets \n",
    "  \n",
    "# metadata \n",
    "print(concrete_compressive_strength.metadata) \n",
    "  \n",
    "# variable information \n",
    "print(concrete_compressive_strength.variables) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_datasets[\"Concrete_Compressive_Strength\"] = {\"X\": X, \"y\": y}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Airfoil Self-Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'uci_id': 291, 'name': 'Airfoil Self-Noise', 'repository_url': 'https://archive.ics.uci.edu/dataset/291/airfoil+self+noise', 'data_url': 'https://archive.ics.uci.edu/static/public/291/data.csv', 'abstract': 'NASA data set, obtained from a series of aerodynamic and acoustic tests of two and three-dimensional airfoil blade sections conducted in an anechoic wind tunnel.', 'area': 'Physics and Chemistry', 'tasks': ['Regression'], 'characteristics': ['Multivariate'], 'num_instances': 1503, 'num_features': 5, 'feature_types': ['Real'], 'demographics': [], 'target_col': ['scaled-sound-pressure'], 'index_col': None, 'has_missing_values': 'no', 'missing_values_symbol': None, 'year_of_dataset_creation': 1989, 'last_updated': 'Fri Mar 29 2024', 'dataset_doi': '10.24432/C5VW2C', 'creators': ['Thomas Brooks', 'D. Pope', 'Michael Marcolini'], 'intro_paper': None, 'additional_info': {'summary': 'The NASA data set comprises different size NACA 0012 airfoils at various wind tunnel speeds and angles of attack. The span of the airfoil and the observer position were the same in all of the experiments. ', 'purpose': None, 'funded_by': None, 'instances_represent': None, 'recommended_data_splits': None, 'sensitive_data': None, 'preprocessing_description': None, 'variable_info': 'This problem has the following inputs:\\r\\n1. Frequency, in Hertzs. \\r\\n2. Angle of attack, in degrees. \\r\\n3. Chord length, in meters.\\r\\n4. Free-stream velocity, in meters per second. \\r\\n5. Suction side displacement thickness, in meters. \\r\\n\\r\\nThe only output is:\\r\\n6. Scaled sound pressure level, in decibels. \\r\\n', 'citation': None}}\n",
      "                                  name     role        type demographic  \\\n",
      "0                            frequency  Feature     Integer        None   \n",
      "1                         attack-angle  Feature      Binary        None   \n",
      "2                         chord-length  Feature  Continuous        None   \n",
      "3                 free-stream-velocity  Feature  Continuous        None   \n",
      "4  suction-side-displacement-thickness  Feature  Continuous        None   \n",
      "5                scaled-sound-pressure   Target  Continuous        None   \n",
      "\n",
      "  description units missing_values  \n",
      "0        None    Hz             no  \n",
      "1        None   deg             no  \n",
      "2        None     m             no  \n",
      "3        None   m/s             no  \n",
      "4        None     m             no  \n",
      "5        None    dB             no  \n"
     ]
    }
   ],
   "source": [
    "# fetch dataset \n",
    "airfoil_self_noise = fetch_ucirepo(id=291) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "X = airfoil_self_noise.data.features \n",
    "y = airfoil_self_noise.data.targets \n",
    "  \n",
    "# metadata \n",
    "print(airfoil_self_noise.metadata) \n",
    "  \n",
    "# variable information \n",
    "print(airfoil_self_noise.variables) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_datasets[\"Airfoil_Self_Noise\"] = {\"X\": X, \"y\": y}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pyrim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = openml.datasets.get_dataset(217)\n",
    "X, y, is_categorical, feat_names = dataset.get_data(\n",
    "    dataset_format=\"dataframe\", target=dataset.default_target_attribute\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_datasets[\"pyrim\"] = {\"X\": X, \"y\": y}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " boston"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = openml.datasets.get_dataset(531)\n",
    "X, y, is_categorical, feat_names = dataset.get_data(\n",
    "    dataset_format=\"dataframe\", target=dataset.default_target_attribute\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_datasets[\"boston\"] = {\"X\": X, \"y\": y}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wine Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'uci_id': 186, 'name': 'Wine Quality', 'repository_url': 'https://archive.ics.uci.edu/dataset/186/wine+quality', 'data_url': 'https://archive.ics.uci.edu/static/public/186/data.csv', 'abstract': 'Two datasets are included, related to red and white vinho verde wine samples, from the north of Portugal. The goal is to model wine quality based on physicochemical tests (see [Cortez et al., 2009], http://www3.dsi.uminho.pt/pcortez/wine/).', 'area': 'Business', 'tasks': ['Classification', 'Regression'], 'characteristics': ['Multivariate'], 'num_instances': 4898, 'num_features': 11, 'feature_types': ['Real'], 'demographics': [], 'target_col': ['quality'], 'index_col': None, 'has_missing_values': 'no', 'missing_values_symbol': None, 'year_of_dataset_creation': 2009, 'last_updated': 'Wed Nov 15 2023', 'dataset_doi': '10.24432/C56S3T', 'creators': ['Paulo Cortez', 'A. Cerdeira', 'F. Almeida', 'T. Matos', 'J. Reis'], 'intro_paper': {'ID': 252, 'type': 'NATIVE', 'title': 'Modeling wine preferences by data mining from physicochemical properties', 'authors': 'P. Cortez, A. Cerdeira, Fernando Almeida, Telmo Matos, J. Reis', 'venue': 'Decision Support Systems', 'year': 2009, 'journal': None, 'DOI': None, 'URL': 'https://www.semanticscholar.org/paper/Modeling-wine-preferences-by-data-mining-from-Cortez-Cerdeira/bf15a0ccc14ac1deb5cea570c870389c16be019c', 'sha': None, 'corpus': None, 'arxiv': None, 'mag': None, 'acl': None, 'pmid': None, 'pmcid': None}, 'additional_info': {'summary': 'The two datasets are related to red and white variants of the Portuguese \"Vinho Verde\" wine. For more details, consult: http://www.vinhoverde.pt/en/ or the reference [Cortez et al., 2009].  Due to privacy and logistic issues, only physicochemical (inputs) and sensory (the output) variables are available (e.g. there is no data about grape types, wine brand, wine selling price, etc.).\\n\\nThese datasets can be viewed as classification or regression tasks.  The classes are ordered and not balanced (e.g. there are many more normal wines than excellent or poor ones). Outlier detection algorithms could be used to detect the few excellent or poor wines. Also, we are not sure if all input variables are relevant. So it could be interesting to test feature selection methods.\\n', 'purpose': None, 'funded_by': None, 'instances_represent': None, 'recommended_data_splits': None, 'sensitive_data': None, 'preprocessing_description': None, 'variable_info': 'For more information, read [Cortez et al., 2009].\\r\\nInput variables (based on physicochemical tests):\\r\\n   1 - fixed acidity\\r\\n   2 - volatile acidity\\r\\n   3 - citric acid\\r\\n   4 - residual sugar\\r\\n   5 - chlorides\\r\\n   6 - free sulfur dioxide\\r\\n   7 - total sulfur dioxide\\r\\n   8 - density\\r\\n   9 - pH\\r\\n   10 - sulphates\\r\\n   11 - alcohol\\r\\nOutput variable (based on sensory data): \\r\\n   12 - quality (score between 0 and 10)', 'citation': None}}\n",
      "                    name     role         type demographic  \\\n",
      "0          fixed_acidity  Feature   Continuous        None   \n",
      "1       volatile_acidity  Feature   Continuous        None   \n",
      "2            citric_acid  Feature   Continuous        None   \n",
      "3         residual_sugar  Feature   Continuous        None   \n",
      "4              chlorides  Feature   Continuous        None   \n",
      "5    free_sulfur_dioxide  Feature   Continuous        None   \n",
      "6   total_sulfur_dioxide  Feature   Continuous        None   \n",
      "7                density  Feature   Continuous        None   \n",
      "8                     pH  Feature   Continuous        None   \n",
      "9              sulphates  Feature   Continuous        None   \n",
      "10               alcohol  Feature   Continuous        None   \n",
      "11               quality   Target      Integer        None   \n",
      "12                 color    Other  Categorical        None   \n",
      "\n",
      "               description units missing_values  \n",
      "0                     None  None             no  \n",
      "1                     None  None             no  \n",
      "2                     None  None             no  \n",
      "3                     None  None             no  \n",
      "4                     None  None             no  \n",
      "5                     None  None             no  \n",
      "6                     None  None             no  \n",
      "7                     None  None             no  \n",
      "8                     None  None             no  \n",
      "9                     None  None             no  \n",
      "10                    None  None             no  \n",
      "11  score between 0 and 10  None             no  \n",
      "12            red or white  None             no  \n"
     ]
    }
   ],
   "source": [
    "# fetch dataset \n",
    "wine_quality = fetch_ucirepo(id=186) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "X = wine_quality.data.features \n",
    "y = wine_quality.data.targets \n",
    "  \n",
    "# metadata \n",
    "print(wine_quality.metadata) \n",
    "  \n",
    "# variable information \n",
    "print(wine_quality.variables) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_datasets[\"Wine_Quality\"] = {\"X\": X, \"y\": y}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "california_housing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20640, 8) (20640,)\n",
      "['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population', 'AveOccup']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "housing = fetch_california_housing()\n",
    "print(housing.data.shape, housing.target.shape)\n",
    "print(housing.feature_names[0:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame for X (features)\n",
    "X = pd.DataFrame(housing.data, columns=housing.feature_names)\n",
    "\n",
    "# Create a Series or DataFrame for y (target)\n",
    "y = pd.Series(housing.target, name='MedHouseVal')  # Median House Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_datasets[\"California_Housing\"] = {\"X\": X, \"y\": y}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a pickle file with the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickle file saved at: Data\\Regression_Original.pkl\n"
     ]
    }
   ],
   "source": [
    "# Create the 'Data' folder if it doesn't exist\n",
    "os.makedirs(\"Data\", exist_ok=True)\n",
    "\n",
    "# Path to save the pickle file\n",
    "file_path = os.path.join(\"Data\", \"Regression_Original.pkl\")\n",
    "\n",
    "# Save the dictionary\n",
    "with open(file_path, \"wb\") as f:\n",
    "    pickle.dump(all_datasets, f)\n",
    "\n",
    "print(f\"Pickle file saved at: {file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Breaking training , validation and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… One-hot encoding done. All datasets split into train/val/test.\n",
      "ðŸ“¦ Saved to 'Data/encoded_split_data.pkl'\n"
     ]
    }
   ],
   "source": [
    "# Initialize dictionary to store splits\n",
    "encoded_split_data = {}\n",
    "\n",
    "# Process each dataset\n",
    "for name, data in all_datasets.items():\n",
    "    X = data[\"X\"].copy()\n",
    "    y = data[\"y\"]\n",
    "\n",
    "    # Step 1: One-hot encode categorical columns\n",
    "    X_encoded = pd.get_dummies(X, drop_first=True)\n",
    "\n",
    "    # Step 2: Train/Test/Val Split\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(X_encoded, y, test_size=0.3, random_state=42)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "    # Step 3: Store into dictionary\n",
    "    encoded_split_data[name] = {\n",
    "        \"X_train\": X_train,\n",
    "        \"X_val\": X_val,\n",
    "        \"X_test\": X_test,\n",
    "        \"y_train\": y_train,\n",
    "        \"y_val\": y_val,\n",
    "        \"y_test\": y_test\n",
    "    }\n",
    "\n",
    "print(\"âœ… One-hot encoding done. All datasets split into train/val/test.\")\n",
    "\n",
    "# Save to pickle\n",
    "os.makedirs(\"Data\", exist_ok=True)\n",
    "with open(\"Data/encoded_split_data.pkl\", \"wb\") as f:\n",
    "    pickle.dump(encoded_split_data, f)\n",
    "\n",
    "print(\"ðŸ“¦ Saved to 'Data/encoded_split_data.pkl'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulating Datasets through MakeRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to Data/Regression_Original_Simulated.pkl\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Initialize output dictionary\n",
    "dataset_dict = {}\n",
    "\n",
    "# Feature counts\n",
    "feature_counts = list(range(10, 151, 10))  # [10,20,30...150]\n",
    "\n",
    "# Generate datasets\n",
    "for n_features in feature_counts:\n",
    "    X, y = make_regression(\n",
    "        n_samples=500,\n",
    "        n_features=n_features,\n",
    "        noise=10.0,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    X_df = pd.DataFrame(X, columns=[f\"f{i+1}\" for i in range(n_features)])\n",
    "    y_df = pd.Series(y, name=\"target\")\n",
    "    \n",
    "    dataset_dict[f\"sim_{n_features}\"] = {\n",
    "        \"X\": X_df,\n",
    "        \"y\": y_df\n",
    "    }\n",
    "\n",
    "# Ensure the Data folder exists\n",
    "os.makedirs(\"Data\", exist_ok=True)\n",
    "\n",
    "# Save the dictionary as a pickle file\n",
    "with open(\"Data/Regression_Original_Simulated.pkl\", \"wb\") as f:\n",
    "    pickle.dump(dataset_dict, f)\n",
    "\n",
    "print(\"Saved to Data/Regression_Original_Simulated.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… All datasets split into train/val/test.\n",
      "ðŸ“¦ Saved to 'Data/encoded_split_data_simulated.pkl'\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Initialize dictionary to store splits\n",
    "encoded_split_data = {}\n",
    "\n",
    "# Process each dataset\n",
    "for name, data in dataset_dict.items():\n",
    "    X = data[\"X\"].copy()\n",
    "    y = data[\"y\"]\n",
    "\n",
    "    # Step 2: Train/Test/Val Split\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "    # Step 3: Store into dictionary\n",
    "    encoded_split_data[name] = {\n",
    "        \"X_train\": X_train,\n",
    "        \"X_val\": X_val,\n",
    "        \"X_test\": X_test,\n",
    "        \"y_train\": y_train,\n",
    "        \"y_val\": y_val,\n",
    "        \"y_test\": y_test\n",
    "    }\n",
    "\n",
    "print(\"âœ… All datasets split into train/val/test.\")\n",
    "\n",
    "# Save to pickle\n",
    "os.makedirs(\"Data\", exist_ok=True)\n",
    "with open(\"Data/encoded_split_data_simulated.pkl\", \"wb\") as f:\n",
    "    pickle.dump(encoded_split_data, f)\n",
    "\n",
    "print(\"ðŸ“¦ Saved to 'Data/encoded_split_data_simulated.pkl'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
